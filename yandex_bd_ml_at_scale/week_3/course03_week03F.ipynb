{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to [@dkaberna](https://github.com/dkaberna) for uploading the solution, I couldn't get pass the error complaining about not using BigData Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cover Type Prediction using ensembles\n",
    "\n",
    "## Dataset Description\n",
    "The dataset represents the data about trees which were planted in the US. The dataset consists of the information about 500000 trees. Your aim is to build  Random Forest Ensemble to predict the cover type of trees. In order to successfully complete this assignment you have to follow this algorithm:\n",
    "* Load the training data\n",
    "* Transform categorical features into vector representations\n",
    "* Split dataset into the train and validation part\n",
    "* Fit the Random Forest Ensemble into the training set\n",
    "* Compare the accuracy of the fitted model with the Logistic Regression Model, which is about 0.67 for this set\n",
    "\n",
    "\n",
    "If you have enough time, it will be very interesting to dig into further research through these steps:\n",
    "* Determine which features are valuable for your model (calculate feature importance of your model).\n",
    "* Try to reduce number of trees and see the results.\n",
    "* Understand why the linear models have poor performance on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals # For the compatibility with Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder\\\n",
    "                            .enableHiveSupport()\\\n",
    "                            .appName(\"spark sql\")\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train dataset located at /data/covertype2 with at least 60 partitions (use function repartition for this case). Use option `inferSchema` to save numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferschema\", \"true\")\\\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "        .load(\"/data/covertype2/train.csv\")\\\n",
    "        .repartition(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two categorical features in dataset: 'Soil_Type' and 'Wild_Type'. You have to transform them into the vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you have to use StringIndexer to transform feature types to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=['Soil_Type','Wild_Type']\n",
    "cat_cols_index={'Soil_Type':'Soil_Index','Wild_Type':'Wild_Index'}\n",
    "cat_cols_encoder={'Soil_Index':'SoilEncoder','Wild_Index':'WildEncoder'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol = \"Soil_Type\", outputCol = \"Soil_Index\")\n",
    "model1 = stringIndexer.fit(df)\n",
    "indexedDF = model1.transform(df)\n",
    "\n",
    "stringIndexer2 = StringIndexer(inputCol = \"Wild_Type\", outputCol = \"Wild_Index\")\n",
    "model2 = stringIndexer2.fit(indexedDF)\n",
    "indexedDF2 = model2.transform(indexedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply OneHotEncoder technique to the dataset in order to get vectors for the Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCol = \"Soil_Index\", outputCol = \"SoilEncoder\")\n",
    "encoder.setDropLast(False)\n",
    "encodedDF = encoder.transform(indexedDF2)\n",
    "\n",
    "encoder2 = OneHotEncoder(inputCol = \"Wild_Index\", outputCol = \"WildEncoder\")\n",
    "encoder2.setDropLast(False)\n",
    "encodedDF2 = encoder2.transform(encodedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the VectorAssembler technique to accumulate all features into one vector. Don't forget to use features that you have generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=['SoilEncoder', # feature name of Soil type encoded\n",
    "                                              'WildEncoder', # feature name of Wild type encoded\n",
    "                                              'Elevation',\n",
    "                                              'Aspect',\n",
    "                                              'Slope',\n",
    "                                              'Horizontal_Distance_To_Hydrology',\n",
    "                                              'Vertical_Distance_To_Hydrology',\n",
    "                                              'Horizontal_Distance_To_Roadways',\n",
    "                                              'Hillshade_9am',\n",
    "                                              'Hillshade_Noon',\n",
    "                                              'Hillshade_3pm',\n",
    "                                              'Horizontal_Distance_To_Fire_Points'\n",
    "                                              ], outputCol='features')\n",
    "finalDF = vector_assembler.transform(encodedDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decisionTree = DecisionTreeClassifier(labelCol = \"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline = Pipeline(stages = [decisionTree])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(decisionTree.maxDepth, [1, 2, 3, 4, 5, 6, 7, 8, 9])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Target\", predictionCol = \"prediction\", metricName = \"accuracy\") \n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = evaluator,\n",
    "                          numFolds = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvModel = crossval.fit(finalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(trainingData, testData) = finalDF.randomSplit([0.8, 0.2], seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions = cvModel.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Target\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Random Forest model to the train dataset. Don't forget to split dataset into two parts to check your trained models. It is desirable to use about 100 trees with depth about 7 in order to avoid wasting too much time waiting while your model will be fit to the data. Try to adjust the options 'subsamplingRate' and 'featureSubsetStrategy' to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Extra task.</b> Use the Cross-Validation to check your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#Pipeline for static model\n",
    "rf = RandomForestClassifier(labelCol='Target',featuresCol='features', numTrees=100,maxDepth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "(trainingData, testData) = finalDF.randomSplit([0.8, 0.2], seed = 123)\n",
    "model = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the feature importances of the trained model. What 5 features are the most important in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model to the validation part of your set and get the accuracy score for the data. Use the MulticlassClassificationEvaluator class from the ml.evaluation module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Target\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addng CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your results better than the results from the Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing test submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the models to the test dataset.\n",
    "\n",
    "<b>Note!</b> Dataset will be changed during the test phase. Your last cell output must be the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, transform dataset\n",
    "dfTest = spark_session.read.format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferschema\", \"true\")\\\n",
    "        .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "        .load(\"/data/covertype2\")\\\n",
    "        .repartition(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = stringIndexer.fit(dfTest)\n",
    "indexedDFTest = model1.transform(dfTest)\n",
    "\n",
    "model2 = stringIndexer2.fit(indexedDFTest)\n",
    "indexedDF2Test= model2.transform(indexedDFTest)\n",
    "\n",
    "encodedDFTest = encoder.transform(indexedDF2Test)\n",
    "\n",
    "encodedDF2Test = encoder2.transform(encodedDFTest)\n",
    "finalDFTest = vector_assembler.transform(encodedDF2Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy for static model\n",
    "predictions = model.transform(finalDFTest)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Target\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7171700726138098\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
