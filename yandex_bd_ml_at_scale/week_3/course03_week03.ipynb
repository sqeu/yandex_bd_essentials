{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction by vacancy description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals # For the compatibility with Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession.builder\\\n",
    "                            .enableHiveSupport()\\\n",
    "                            .appName(\"spark sql\")\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train dataset placed at `/data/vacancie` with at least 10 partitions (use function `repartition` for this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "        StructField(\"Elevation\", IntegerType(), False),\n",
    "        StructField(\"Aspect\", IntegerType(), False),\n",
    "        StructField(\"Slope\", IntegerType(), False),\n",
    "       StructField(\"Horizontal_Distance_To_Hydrology\", IntegerType(), False),\n",
    "       StructField(\"Vertical_Distance_To_Hydrology\", IntegerType(), False),\n",
    "       StructField(\"Horizontal_Distance_To_Roadways\", IntegerType(), False),\n",
    "       StructField(\"Hillshade_9am\", IntegerType(), False),\n",
    "       StructField(\"Hillshade_Noon\", IntegerType(), False),\n",
    "       StructField(\"Hillshade_3pm\", IntegerType(), False),\n",
    "       StructField(\"Horizontal_Distance_To_Fire_Points\", IntegerType(), False),\n",
    "       StructField(\"Wild_Type\", StringType(), False),\n",
    "       StructField(\"Soil_Type\", StringType(), False),\n",
    "       StructField(\"Target\", IntegerType(), False)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = spark_session.read.csv(\"/data/covertype2\", header=\"true\",schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+---------+---------------------------------------------------------------------------+------+\n",
      "|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points|Wild_Type|Soil_Type                                                                  |Target|\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+---------+---------------------------------------------------------------------------+------+\n",
      "|3122     |266   |10   |433                             |75                            |3069                           |195          |245           |188          |451                               |Comanche |Catamount family - Rock outcrop - Leighcan family complex, extremely stony.|1     |\n",
      "|3018     |308   |15   |60                              |14                            |5359                           |177          |229           |192          |4546                              |Rawah    |Como - Legault families complex, extremely stony.                          |1     |\n",
      "+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+---------+---------------------------------------------------------------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trees.show(2,truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol = \"Wild_Type\",outputCol=\"Wild_Type_si\")\n",
    "trees_2 = stringIndexer.fit(trees).transform(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer2 = StringIndexer(inputCol = \"Soil_Type\",outputCol=\"Soil_Type_si\")\n",
    "trees_3 = stringIndexer2.fit(trees_2).transform(trees_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Wild_Type_si\", outputCol=\"Wild_Type_oh\")\n",
    "tree_oh = encoder.transform(trees_3)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"Soil_Type_si\", outputCol=\"Soil_Type_oh\")\n",
    "tree_oh_2 = encoder.transform(tree_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Elevation\",\"Aspect\",\"Slope\",\"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\",\"Wild_Type_oh\",\"Soil_Type_oh\"], outputCol=\"features\")\n",
    "\n",
    "data = assembler.transform(tree_oh_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data.select(\"target\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train and validation part (it is better to use 90% for the train part and 10% for the validation part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|target|features                                                                                                 |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,11,15],[3122.0,266.0,10.0,433.0,75.0,3069.0,195.0,245.0,188.0,451.0,1.0,1.0])   |\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,10,13],[3018.0,308.0,15.0,60.0,14.0,5359.0,177.0,229.0,192.0,4546.0,1.0,1.0])   |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,10,13],[3146.0,151.0,12.0,541.0,-2.0,5887.0,236.0,240.0,132.0,1371.0,1.0,1.0])  |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,10,13],[2980.0,163.0,6.0,553.0,21.0,3538.0,226.0,242.0,149.0,1087.0,1.0,1.0])   |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,10,19],[2972.0,187.0,16.0,255.0,109.0,6390.0,220.0,250.0,158.0,4119.0,1.0,1.0]) |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,11,22],[2768.0,17.0,13.0,30.0,4.0,3140.0,209.0,213.0,139.0,875.0,1.0,1.0])      |\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,11,15],[2948.0,319.0,9.0,42.0,7.0,2160.0,197.0,232.0,175.0,518.0,1.0,1.0])      |\n",
      "|6     |(52,[0,1,2,3,4,5,6,7,8,9,12,18],[2127.0,320.0,31.0,67.0,40.0,858.0,127.0,195.0,200.0,845.0,1.0,1.0])     |\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,10,13],[2968.0,322.0,19.0,127.0,24.0,3942.0,167.0,217.0,188.0,1248.0,1.0,1.0])  |\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,17],[2983.0,295.0,10.0,124.0,10.0,323.0,192.0,237.0,185.0,190.0,1.0])           |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,11,16],[2947.0,54.0,22.0,182.0,29.0,603.0,227.0,189.0,85.0,1806.0,1.0,1.0])     |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,10,13],[2987.0,130.0,16.0,150.0,26.0,3789.0,244.0,229.0,109.0,2691.0,1.0,1.0])  |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,11,27],[2748.0,8.0,14.0,95.0,10.0,603.0,203.0,212.0,145.0,2358.0,1.0,1.0])      |\n",
      "|7     |(52,[0,1,2,3,4,5,6,7,8,9,10,24],[3330.0,81.0,18.0,190.0,-4.0,2058.0,242.0,205.0,86.0,3273.0,1.0,1.0])    |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,11,16],[2944.0,12.0,13.0,256.0,65.0,1528.0,206.0,212.0,142.0,2630.0,1.0,1.0])   |\n",
      "|7     |(52,[0,1,2,5,6,7,8,9,11,16],[3381.0,203.0,10.0,4260.0,215.0,250.0,168.0,3565.0,1.0,1.0])                 |\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,11,16],[3083.0,199.0,13.0,277.0,28.0,891.0,215.0,251.0,168.0,2989.0,1.0,1.0])   |\n",
      "|7     |(52,[0,1,2,3,4,5,6,7,8,9,11,29],[3451.0,279.0,17.0,1167.0,324.0,4116.0,170.0,240.0,208.0,2654.0,1.0,1.0])|\n",
      "|1     |(52,[0,1,2,3,4,5,6,7,8,9,11,22],[3202.0,51.0,19.0,457.0,99.0,3097.0,226.0,197.0,98.0,930.0,1.0,1.0])     |\n",
      "|2     |(52,[0,1,2,3,4,5,6,7,8,9,11,27],[2900.0,221.0,14.0,450.0,119.0,1320.0,202.0,253.0,185.0,1655.0,1.0,1.0]) |\n",
      "+------+---------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData,testData = data_final.randomSplit([0.7,0.3],seed = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Logistic Regression to the model on the splitted train part. Use about 15 iterations for the training process.\n",
    "\n",
    "<b>Hint.</b> Use regularization parameter in order to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='target',featuresCol= \"features\",numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rfModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6469443319580774"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"target\", predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Performing test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform dataset and calculate auc-roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output for the AUC-ROC"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
