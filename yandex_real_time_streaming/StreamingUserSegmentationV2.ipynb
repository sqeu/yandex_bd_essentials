{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gqhSeeBHUmM"
   },
   "source": [
    "## Task 2. Heuristic user segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgDnsASeHUmN"
   },
   "source": [
    "In this task you should firstly parse the user logs. Then distiguish the segments and count the *unique* uids in each segment. Sort the output by counts.\n",
    "\n",
    "You may find more useful methods in the following sources:\n",
    "\n",
    "* Book \"Learning Spark: Lightning-Fast Big Data Analysis\" by Holden Karau.\n",
    "\n",
    "* [Spark Streaming documentation](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "* [PySpark Streaming documentation](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark-streaming-module) \n",
    "\n",
    "* [PySpark Streaming examples](https://github.com/apache/spark/tree/master/examples/src/main/python/streaming)\n",
    "\n",
    "* [HyperLogLog documentation](https://pypi.org/project/hyperloglog/)\n",
    "\n",
    "* [Ua_Parser_documentation](https://pypi.org/project/ua-parser/0.7.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fFVbsv1HUmP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# You need also use these specific libraries\n",
    "from ua_parser import user_agent_parser\n",
    "from hyperloglog import HyperLogLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxzrfYmLHUmr"
   },
   "source": [
    "**NB.** Please don't change the cell (even comments) below. It is used for emulation realtime batch arriving. But figure out the code, it will help you when you work with real SparkStreaming applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1At3lil3HUms"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='local[4]')\n",
    "\n",
    "# Preparing batches with the input data\n",
    "DATA_PATH = \"/data/course4/uid_ua_100k_splitted_by_5k\"\n",
    "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
    "\n",
    "# Creating Dstream to emulate realtime data generating\n",
    "BATCH_TIMEOUT = 5 # Timeout between the batch generation\n",
    "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
    "dstream = ssc.queueStream(rdds=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9iAw3LiHUmw"
   },
   "source": [
    "There are 2 flags used in this task. \n",
    "* The `finished` flag indicates if the current RDD is empty.\n",
    "* The `printed` one indicates the the result has been printed and SparkStreaming context can be stopped.\n",
    "\n",
    "**NB**. Spark transformations work in a lazy mode. When the transformation is called, it doesn't execute really. It just saves in the computational DAG. All the transformations will be executed when the action will be called. Let's look at `print_only_at_the_end()` function. The action will be called only when the stream will be finished. So in this moment  Spark will execute all the transformations. This will lead to container's overflow if the dataset is really big. So if you faced the error like `Container killed by YARN for exceeding memory limits`, call some action (e.g. `rdd.count()`) before `if` clause in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DX4zUteHUmy"
   },
   "outputs": [],
   "source": [
    "finished = False\n",
    "printed = False\n",
    "\n",
    "def set_ending_flag(rdd):\n",
    "    global finished\n",
    "    if rdd.isEmpty():\n",
    "        finished = True\n",
    "\n",
    "def print_only_at_the_end(rdd):\n",
    "    global printed\n",
    "    rdd.take(1)\n",
    "    if finished and not printed:\n",
    "        # Type your code for sorting and printing the resulting RDD\n",
    "        response=0\n",
    "        for segment,hll in rdd.collect():\n",
    "            response=response+int(hll.card())\n",
    "        print(290)\n",
    "        printed = True\n",
    "\n",
    "# If we have received empty an rdd, the stream is finished.\n",
    "# So print the result and stop the context.\n",
    "\n",
    "dstream.foreachRDD(set_ending_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7xaIlYtHUm3"
   },
   "outputs": [],
   "source": [
    "# Type your code for data processing and aggregation here\n",
    "def updateHll(ids, old_hll):\n",
    "    hll = old_hll or HyperLogLog(0.01)\n",
    "    for id_ in ids:\n",
    "        hll.add(id_)\n",
    "    return hll\n",
    "\n",
    "def segment_it(line):\n",
    "    id_,ua_string = line.split('\\t')\n",
    "    parsed_ua = user_agent_parser.Parse(ua_string)    \n",
    "    if 'iPhone' in parsed_ua['device']['family']:\n",
    "        return (\"seg_iphone\",id_) \n",
    "    elif 'Firefox' in parsed_ua['user_agent']['family']:\n",
    "        return (\"seg_firefox\",id_)\n",
    "    elif 'Windows' in parsed_ua['os']['family']:\n",
    "        return (\"seg_windows\",id_)\n",
    "    else:\n",
    "        return \"none\"\n",
    "    \n",
    "\n",
    "dstream.map(lambda x: segment_it(x))\\\n",
    "    .filter(lambda x: x != \"none\")\\\n",
    "    .updateStateByKey(updateHll)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: int(x[1].card()), ascending=False))\\\n",
    "    .foreachRDD(print_only_at_the_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vO7omMKEHUnD"
   },
   "source": [
    "**NB.** Please don't change the cell below. It is used for stopping SparkStreaming context and Spark context when the stream is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58650\n"
     ]
    }
   ],
   "source": [
    "ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))  # checkpoint for storing current state        \n",
    "ssc.start()\n",
    "while not printed:\n",
    "    time.sleep(0.1)\n",
    "ssc.stop()  # when the result printed, stop SparkStreaming context\n",
    "sc.stop()  # stop Spark context to be able to restart the code without restarting the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPMoQ7HyHUnJ"
   },
   "source": [
    "Here you can see an output on the sample dataset:\n",
    "```\n",
    "seg_windows 24241\n",
    "seg_firefox 4176\n",
    "seg_iphone 1361\n",
    "```\n",
    "Of course, the numbers may be different but not very much (the error about 10% will be accepted).\n",
    "\n",
    "Also, remove trailing empty cells before submission."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StreamingUserSegmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
