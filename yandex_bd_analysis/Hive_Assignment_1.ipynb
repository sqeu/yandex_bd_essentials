{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfiL39OTJ5sR"
   },
   "source": [
    "# Hive assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VD19-0y3J5sR"
   },
   "source": [
    "The purpose of this task is to create an external table on the posts data of the `stackoverflow.com` website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKx3tQ4_J5sS"
   },
   "source": [
    "## Step 1. Intro. Creation of the DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10mBFHfPJ5sS"
   },
   "source": [
    "Let's create the sandbox database where you will complete your assignment.\n",
    "\n",
    "<b>Note!</b> This code shouldn't be in your submission. Please, remove this code from the notebook before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v1kw1lFXJ5sU",
    "outputId": "7fa089c4-ee8f-40ec-b220-5101298f1b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile creation_db.hql\n",
    "DROP DATABASE IF EXISTS demodb CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gk869885J5sZ",
    "outputId": "7c890932-c8de-41ba-ad23-f3b966198693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to creation_db.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a creation_db.hql\n",
    "CREATE DATABASE demodb LOCATION '/user/jovyan/test_metastore';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JNqLknFJJ5sc",
    "outputId": "c5b66828-649f-408d-9651-403cff75522c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Exception in thread \"main\" java.lang.RuntimeException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/mydir/jovyan/c1bc6316-9264-452e-bcbd-1371ec7a8fe7. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:472)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:234)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\n",
      "Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/mydir/jovyan/c1bc6316-9264-452e-bcbd-1371ec7a8fe7. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2527)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2500)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1155)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1152)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1152)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1144)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.createPath(SessionState.java:584)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:526)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:458)\n",
      "\t... 8 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/mydir/jovyan/c1bc6316-9264-452e-bcbd-1371ec7a8fe7. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1481)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1337)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:574)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n",
      "\tat com.sun.proxy.$Proxy16.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2525)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "! hive -f creation_db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvsrnOe6J5se"
   },
   "source": [
    "**Don't forget to remove this code before submission!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPEiXPuAJ5sf"
   },
   "source": [
    "## Step 2. Exploration of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdzmqWgQJ5sg"
   },
   "source": [
    "Okay, we have created the database. Let's create your own table for users and posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li8EZVxtJ5sg"
   },
   "source": [
    "First of all, let's watch at the datasets for `users` which are located at `/data/stackexchange1000/posts` and for `posts` which is located at `/data/stackexchange1000/users`. Print the first three rows of those datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HBbrpNdIeM6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "reporter:status:Reading \t\n",
      "<row Id=\"1394\" PostTypeId=\"2\" ParentId=\"1390\" CreationDate=\"2008-08-04T16:38:03.667\" Score=\"16\" Body=\"&lt;p&gt;Not sure how credible &lt;a href=&quot;http://www.builderau.com.au/program/windows/soa/Getting-started-with-Windows-Server-2008-Core-edition/0,339024644,339288700,00.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;this source is&lt;/a&gt;, but:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The Windows Server 2008 Core edition can:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the file server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Hyper-V virtualization server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Directory Services role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DHCP server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the IIS Web server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DNS server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Active Directory Lightweight Directory Services.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the print server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;The Windows Server 2008 Core edition cannot:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run a SQL Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run an Exchange Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Internet Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Windows Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Host a remote desktop session.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run MMC snap-in consoles locally.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;\" OwnerUserId=\"91\" LastEditorUserId=\"1\" LastEditorDisplayName=\"Jeff Atwood\" LastEditDate=\"2008-08-27T13:02:50.273\" LastActivityDate=\"2008-08-27T13:02:50.273\" CommentCount=\"1\" />\t\n",
      "<row Id=\"3543\" PostTypeId=\"2\" ParentId=\"3530\" CreationDate=\"2008-08-06T15:24:00.787\" Score=\"37\" Body=\"&lt;p&gt;from &lt;a href=&quot;http://www.timocracy.com/articles/2008/02/21/calling-invoking-rails-rake-tasks-from-within-ruby-for-testing-try-2&quot; rel=&quot;nofollow noreferrer&quot;&gt;timocracy.com&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;require 'rake'&#xA;require 'rake/rdoctask'&#xA;require 'rake/testtask'&#xA;require 'tasks/rails'&#xA;&#xA;def capture_stdout&#xA;  s = StringIO.new&#xA;  oldstdout = $stdout&#xA;  $stdout = s&#xA;  yield&#xA;  s.string&#xA;ensure&#xA;  $stdout = oldstdout&#xA;end&#xA;&#xA;Rake.application.rake_require '../../lib/tasks/metric_fetcher'&#xA;results = capture_stdout {Rake.application['metric_fetcher'].invoke}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;\" OwnerUserId=\"399\" LastActivityDate=\"2008-08-06T15:24:00.787\" CommentCount=\"3\" />\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Your code here. Print the first three rows of posts\n",
    "!hdfs dfs -cat /data/stackexchange1000/posts/part-00000 | head -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bRiLxzoReZ7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "reporter:status:Reading \t\n",
      "<row Id=\"756\" Reputation=\"2358\" CreationDate=\"2008-08-08T15:31:50.013\" DisplayName=\"Simon Gillbee\" LastAccessDate=\"2016-12-09T15:38:03.453\" WebsiteUrl=\"http://simon.gillbee.com\" Location=\"Pearland, TX\" AboutMe=\"&lt;p&gt;Personally, I am a husband, step-father, grandfather, Christ-worshiper, singer, worship leader, computer programmer, reader, game player, kite flyer, generally all around fun guy (or that fungi?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Professionally, I've been developing both commercial and proprietary systems for the last 20 years. These days I'm primarily writing enterprise-scale client and server software using .NET and loving it!&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;SOreadytohelp&lt;/h1&gt;&#xA;\" Views=\"478\" UpVotes=\"352\" DownVotes=\"25\" Age=\"45\" AccountId=\"587\" />\t\n",
      "<row Id=\"2050\" Reputation=\"4177\" CreationDate=\"2008-08-20T00:32:49.217\" DisplayName=\"Eric Platon\" LastAccessDate=\"2016-12-10T22:24:27.217\" WebsiteUrl=\"\" Location=\"Tokyo, Japan\" AboutMe=\"\" Views=\"387\" UpVotes=\"1045\" DownVotes=\"6\" Age=\"46\" AccountId=\"1533\" />\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# Your code here. Print the first three rows of users\n",
    "!hdfs dfs -cat /data/stackexchange1000/users/part-00000 | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGrOQYgqJ5sn"
   },
   "source": [
    "As you can see, those rows contain some information about posts and users in XML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoNccKriJ5sp"
   },
   "source": [
    "<b>Question.</b> Which fields for users and posts do you think are the most important for the analysis? And for joining tables? \n",
    "OwnerUserId from Posts\n",
    "Id from Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mWC_LHQJ5sr"
   },
   "source": [
    "<h3><b>Please, check your answer with this information!</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3hcl3lLyJ5sq"
   },
   "source": [
    "So, the lines not started with the \"row\" tags should be ignored. The valid row contains the following fields and their order is not defined:\n",
    "\n",
    "* Id (integer) - id of the post\n",
    "* PostTypeId (integer: 1 or 2) - 1 for questions, 2 for answers\n",
    "* CreationDate (date) - post creation date in the format \"YYYY-MM-DDTHH:MM:SS.ms\"\n",
    "* Tags (string, optional) - list of post tags, each tag is wrapped with html entities `&lt;` and `&gt;`\n",
    "* OwnerUserId (integer, optional) - user id of the post's author\n",
    "* ParentId (integer, optional) - for answers - id of the question\n",
    "* Score (integer) - score (votes) of a question or an answer, can be negative (!)\n",
    "* FavoriteCount (integer, optional) - how many times the question was added in the favorites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmQ2vtOVJ5sr"
   },
   "source": [
    "The second part of the dataset contains StackOverflow users.\n",
    "\n",
    "The fields are the following and their order is also not defined:\n",
    "\n",
    "* Id (integer) - user id\n",
    "* Reputation (integer) - user's reputation\n",
    "* CreationDate (string) - creation date in the format \"YYYY-MM-DDTHH:MM:SS.ms\"\n",
    "* DisplayName (string) - user's name\n",
    "* Location (string, options) - user's country\n",
    "* Age (integer, optional) - user's age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Na2THNHnJ5ss"
   },
   "source": [
    "## Step 3. Train your regexp skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxFZQpxIJ5st"
   },
   "source": [
    "In this step you will find out how to parse information for complex rows! You will try to create some examples for parsing! There are some general rules for parsing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRaJBzTeJ5su"
   },
   "source": [
    "1. To create a regular expression, which describes strings containing two patterns, where the order of the patterns is not defined use the following so-called ‘positive lookahead assertion’ with `?=` group modifier. For example, both strings “Washington Irving” and “Irving Washington” match the pattern:\n",
    "```\n",
    "(?=.*Washington)(?=.*Irving)\n",
    "```.\n",
    "2. To capture groups use round brackets. So, the pattern: `(?=.*(Washington))(?=.*(Irving))` captures `Washington` and `Irving` from both strings: \"William Arthur Irving Washington was an English first-class Cricketer\" and: “Washington Irving was an American writer”.\n",
    "3. Use `\\b` to specify boundaries of words and increase accuracy of your pattern. For example: pattern `(?=.*\\bID=(\\d+))(?=.*\\bUserID=(\\d+))` captures `1` and `2` from the string `ID=1 UserID=2`, whereas pattern without `\\b`: `(?=.*ID=(\\d+))(?=.*UserID=(\\d+))` returns the wrong groups: `2` and `2`.\n",
    "4. In Hive pattern for the external table in SERDEPROPERTIES `input.regex` should describe the whole input string, add `.*` at the end of the pattern.\n",
    "5. Don't forget that for the beginning of string should also be covered. That's why use the pattern `.*?` for lazy initialization of future patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cRobnWBYJ5sv"
   },
   "source": [
    "To sum up, you can create your first regex for parsing Id from posts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fA7LBThaJ5sv"
   },
   "source": [
    "<b>Question!</b> What will be your first regex for parsing Id from the posts? Don't forget to add steps 4 and 5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAwyKE9EJ5sw"
   },
   "source": [
    "<div class=\"panel-group\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" href=\"#collapse-answer\">Check your answer!</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-answer\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">The correct answer is `\".*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*\"` </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pirvGNs1J5sx"
   },
   "source": [
    "Let's create the first external table with one row which contains only `Id` field. Let's name it `posts_external_only_id`.\n",
    "You can watch the lecture for the SerDe format: <a href=\"https://www.coursera.org/learn/big-data-analysis/lecture/wAGe6/hive-analytics-regexserde-views\">Serde Format</a> and <a href=\"/notebooks/demos/course02_week02-Demo_submission.ipynb#2.-Creation-the-external-table\">creation of external table</a> tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bMdai9W-J5sx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo_example.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo_example.hql\n",
    "\n",
    "-- adding necessary JARs and including database\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_external_only_id;\n",
    "\n",
    "\n",
    "-- Create external table\n",
    "\n",
    "-- Your code here\n",
    "CREATE EXTERNAL TABLE posts_external_only_id \n",
    "(Id string) \n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES ('input.regex'='.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*') \n",
    "LOCATION '/data/stackexchange1000/posts'; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zuwW8QeTJ5s4",
    "outputId": "0d1b9829-7abf-45e9-cdd3-37bffc6db60f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar]\n",
      "OK\n",
      "Time taken: 1.111 seconds\n",
      "OK\n",
      "Time taken: 2.16 seconds\n",
      "OK\n",
      "Time taken: 0.58 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f demo_example.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lNqp7uWJ5s9"
   },
   "source": [
    "Hooray! You have created your first table. Let us watch for this table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z4m1NcvkJ5s9",
    "outputId": "fb1d255e-e8e6-495d-bdb9-aef62dce66d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting describe.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile describe.hql\n",
    "\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DESCRIBE posts_external_only_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t0XIXEHgJ5tB",
    "outputId": "0bee136b-e500-4cad-d817-da3115b84937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.82 seconds\n",
      "OK\n",
      "id                  \tstring              \t                    \n",
      "Time taken: 1.133 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f describe.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuvkMUIqJ5tE"
   },
   "source": [
    "Let's see that the data is  correctly parsed. For this case, take a select query that chooses for us first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZkONpnbTJ5tF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_first_select.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_first_select.hql\n",
    "-- Your code here\n",
    "SELECT * FROM demodb.posts_external_only_id LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "imN6eicpJ5tJ",
    "outputId": "a14b9ad1-944c-4184-966a-aa38b2cb1377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "NULL\n",
      "1394\n",
      "3543\n",
      "4521\n",
      "8689\n",
      "9062\n",
      "14671\n",
      "16307\n",
      "18780\n",
      "18929\n",
      "Time taken: 2.754 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f my_first_select.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFL5R6oTJ5tO"
   },
   "source": [
    "How many posts are there in the dataset? Don't forget to clear the `NULL` values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "d0gg-zzyJ5tO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting how_many_posts.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile how_many_posts.hql\n",
    "-- Your code here\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "select count(*) from demodb.posts_external_only_id where id is not null;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "68Zo73CjJ5tT",
    "outputId": "df326357-6789-481e-89b4-bdc8ae8c47b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "Query ID = jovyan_20190928182727_e5ab0644-3e0d-4922-b2e1-8f5c826bd4f9\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1569693762567_0002, Tracking URL = http://9eec40d2ff6f:8088/proxy/application_1569693762567_0002/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1569693762567_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-09-28 18:28:01,777 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-09-28 18:28:22,106 Stage-1 map = 47%,  reduce = 0%, Cumulative CPU 8.43 sec\n",
      "2019-09-28 18:28:24,364 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.57 sec\n",
      "2019-09-28 18:28:36,815 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 12.72 sec\n",
      "MapReduce Total cumulative CPU time: 12 seconds 720 msec\n",
      "Ended Job = job_1569693762567_0002\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 12.72 sec   HDFS Read: 60006430 HDFS Write: 6 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 12 seconds 720 msec\n",
      "OK\n",
      "39047\n",
      "Time taken: 57.675 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f how_many_posts.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjP7lEGZJ5tW"
   },
   "source": [
    "Try to parse different fields: for example, day and month of creation date. Don't forget that Hive will accept values for the capturing group in the lookahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ai72gzOJ5tX"
   },
   "source": [
    "Now you are ready to complete your task! Before this you can check your regular expression for parsing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RK4ni3GdJ5tY"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yT8UyDAKJ5tZ"
   },
   "outputs": [],
   "source": [
    "CHECK_ROW = '<row Id=\"1394\" PostTypeId=\"2\" ParentId=\"1390\" CreationDate=\"2008-08-04T16:38:03.667\" Score=\"16\" Body=\"&lt;p&gt;Not sure how credible &lt;a href=&quot;http://www.builderau.com.au/program/windows/soa/Getting-started-with-Windows-Server-2008-Core-edition/0,339024644,339288700,00.htm&quot; rel=&quot;nofollow noreferrer&quot;&gt;this source is&lt;/a&gt;, but:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;The Windows Server 2008 Core edition can:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the file server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Hyper-V virtualization server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the Directory Services role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DHCP server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the IIS Web server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the DNS server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Active Directory Lightweight Directory Services.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run the print server role.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;  &#xA;  &lt;p&gt;The Windows Server 2008 Core edition cannot:&lt;/p&gt;&#xA;  &#xA;  &lt;ul&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run a SQL Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run an Exchange Server.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Internet Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run Windows Explorer.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Host a remote desktop session.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;li&gt;&lt;p&gt;Run MMC snap-in consoles locally.&lt;/p&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/blockquote&gt;&#xA;\" OwnerUserId=\"91\" LastEditorUserId=\"1\" LastEditorDisplayName=\"Jeff Atwood\" LastEditDate=\"2008-08-27T13:02:50.273\" LastActivityDate=\"2008-08-27T13:02:50.273\" CommentCount=\"1\" />'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CGLH3FM_J5tb"
   },
   "outputs": [],
   "source": [
    "CHECK_REGEX = '^<row.*?(?=.*\\\\bId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*'\n",
    "CHECK_REGEX = '.*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*)-(\\\\d*).*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "P8ZiiwbqJ5tf"
   },
   "outputs": [],
   "source": [
    "result = re.match(CHECK_REGEX, CHECK_ROW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lKWMRzJNJ5ti"
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert result.group(0) == CHECK_ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JeTfyTUwJ5tk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2008', '08', '04')\n"
     ]
    }
   ],
   "source": [
    "# Check that your groups are correct\n",
    "print(result.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agtKfzx2J5tm"
   },
   "source": [
    "## Step 4. Complete the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jRdQeLwkJ5tn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_external_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_external_table.hql\n",
    "-- Create external table posts_sample_external with suitable values\n",
    "-- Your code here\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-serde.jar;\n",
    "#(?=.*\\\\bTags=\\\"(\\\\S+)\\\")?(?=.*\\\\bOwnerUserId=\\\"(\\\\d+)\\\")?(?=.*\\\\bParentId=\\\"(\\\\d+)\\\")?(?=.*\\\\bScore=\\\"(\\\\d+)\\\")(?=.*\\\\bFavoriteCount=\\\"(\\\\d+)\\\")?\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "CREATE EXTERNAL TABLE posts_sample_external \n",
    "(Id int, \n",
    " year int,\n",
    " month int,\n",
    " PostTypeId int,\n",
    " Tags string,\n",
    " OwnerUserId int,\n",
    " ParentId int,\n",
    " Score int,\n",
    " FavoriteCount int\n",
    ") \n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' \n",
    "WITH SERDEPROPERTIES ('input.regex'='^<row.*(?=.*\\\\bId=\\\"(\\\\d+)\\\")(?=.*\\\\bPostTypeId=\\\"(\\\\d+)\\\").*(?<=\\\\bCreationDate\\\\b=\\\")(\\\\d*)-(\\\\d*).*') \n",
    "LOCATION '/data/stackexchange1000/posts';   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "j8r1_LvGJ5tr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Exception in thread \"main\" java.lang.RuntimeException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/mydir/jovyan/7ac67d31-e481-4e7e-82ed-ee26dd17d137. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:472)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)\n",
      "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:234)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\n",
      "Caused by: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/mydir/jovyan/7ac67d31-e481-4e7e-82ed-ee26dd17d137. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n",
      "\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2527)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2500)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1155)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1152)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1152)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1144)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.createPath(SessionState.java:584)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:526)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:458)\n",
      "\t... 8 more\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot create directory /tmp/mydir/jovyan/7ac67d31-e481-4e7e-82ed-ee26dd17d137. Name node is in safe mode.\n",
      "The reported blocks 931 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 934.\n",
      "The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1372)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1360)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:2969)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1078)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:637)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1481)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1427)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1337)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n",
      "\tat com.sun.proxy.$Proxy15.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:574)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n",
      "\tat com.sun.proxy.$Proxy16.mkdirs(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2525)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "!hive -f task1_create_external_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaImIBNKJ5ts"
   },
   "source": [
    "Make sure that you have created your table correctly. Select the first 10 posts from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2LdLWs-cJ5tt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_check_select.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_check_select.hql\n",
    "\n",
    "-- Write select query for the first 10 rows\n",
    "-- Your code here\n",
    "SELECT * FROM demodb.posts_sample_external LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rIsmJ-uqJ5tu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "NULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\tNULL\n",
      "1394\t2\t2008\t8\tNULL\t91\tNULL\t16\tNULL\n",
      "3543\t2\t2008\t8\tNULL\t399\tNULL\t37\tNULL\n",
      "4521\t2\t2008\t8\tNULL\t358\tNULL\t25\tNULL\n",
      "8689\t2\t2008\t8\tNULL\t1087\tNULL\t1\tNULL\n",
      "9062\t2\t2008\t8\tNULL\t198\tNULL\t1\tNULL\n",
      "14671\t2\t2008\t8\tNULL\t414\tNULL\t1\tNULL\n",
      "16307\t2\t2008\t8\tNULL\t17\tNULL\t2\tNULL\n",
      "18780\t2\t2008\t8\tNULL\t414\tNULL\t0\tNULL\n",
      "18929\t2\t2008\t8\tNULL\t2031\tNULL\t13\tNULL\n",
      "Time taken: 2.236 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "!hive -f task1_check_select.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqfA8StqJ5tx"
   },
   "source": [
    "Create managed table `posts_sample`. Create the partition by the month and by the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0N7DS8HaJ5tx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_create_managed_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_create_managed_table.hql\n",
    "-- create managed table\n",
    "-- Check that this table contains info about year and month\n",
    "-- Your code here\n",
    "ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar;\n",
    "\n",
    "USE demodb;\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "CREATE TABLE posts_sample \n",
    "(Id int, \n",
    " PostTypeId int,\n",
    " Tags string,\n",
    " OwnerUserId int,\n",
    " ParentId int,\n",
    " Score int,\n",
    " FavoriteCount int\n",
    ") \n",
    "PARTITIONED BY(year string,month string);   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bfwzAFdEJ5tz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Added [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar] to class path\n",
      "Added resources: [/opt/cloudera/parcels/CDH/lib/hive/lib/hive-contrib.jar]\n",
      "OK\n",
      "Time taken: 0.8 seconds\n",
      "OK\n",
      "Time taken: 2.374 seconds\n",
      "OK\n",
      "Time taken: 0.86 seconds\n"
     ]
    }
   ],
   "source": [
    "!hive -f task1_create_managed_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUNkq4SXJ5t0"
   },
   "source": [
    "Populate data from the table `posts_sample_external` to the table `posts_sample`. Don't forget about the partitioning rules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HdCM4rS7J5t0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_insert_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_insert_table.hql\n",
    "\n",
    "-- Insert data to the managed table\n",
    "\n",
    "\n",
    "USE demodb;\n",
    "-- filling managed posts table from external one\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "-- Your code here for inserting data\n",
    "set hive.exec.max.dynamic.partitions=1024;\n",
    "set hive.exec.max.dynamic.partitions.pernode=512;\n",
    "INSERT OVERWRITE TABLE POSTS_SAMPLE \n",
    "PARTITION (year,month)\n",
    "SELECT Id,PostTypeId,year,month,Tags, OwnerUserId, ParentId, Score, FavoriteCount \n",
    "FROM posts_sample_external\n",
    "WHERE YEAR IS NOT NULL and month IS NOT NULL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3V9Ryu0XJ5t3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.905 seconds\n",
      "Query ID = jovyan_20190928212020_0812a975-8a24-46ad-83b7-180bb68c0ce3\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1569693762567_0007, Tracking URL = http://9eec40d2ff6f:8088/proxy/application_1569693762567_0007/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1569693762567_0007\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-09-28 21:20:49,162 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-09-28 21:21:39,272 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 37.12 sec\n",
      "2019-09-28 21:22:39,467 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 90.79 sec\n",
      "2019-09-28 21:22:58,289 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 107.4 sec\n",
      "2019-09-28 21:23:40,182 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 145.49 sec\n",
      "2019-09-28 21:24:40,387 Stage-1 map = 3%,  reduce = 0%, Cumulative CPU 194.23 sec\n",
      "2019-09-28 21:24:45,625 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 199.58 sec\n",
      "2019-09-28 21:25:46,455 Stage-1 map = 4%,  reduce = 0%, Cumulative CPU 254.41 sec\n",
      "2019-09-28 21:26:04,168 Stage-1 map = 5%,  reduce = 0%, Cumulative CPU 270.98 sec\n",
      "^C\n",
      "killing job with: job_1569693762567_0007\n"
     ]
    }
   ],
   "source": [
    "!hive -f task1_insert_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QZWORJwJ5t4"
   },
   "source": [
    "Make sure that your table contains appropriate data about posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task1_watch_new_table.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile task1_watch_new_table.hql\n",
    "-- Your code here\n",
    "SELECT distinct year,month FROM demodb.posts_sample_external;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "Query ID = jovyan_20190928212828_ff9d45c0-d1d5-4d89-a322-c74af185da22\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1569693762567_0008, Tracking URL = http://9eec40d2ff6f:8088/proxy/application_1569693762567_0008/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1569693762567_0008\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-09-28 21:28:37,995 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-09-28 21:29:38,916 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 42.8 sec\n",
      "2019-09-28 21:29:46,761 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 51.2 sec\n",
      "2019-09-28 21:30:47,773 Stage-1 map = 1%,  reduce = 0%, Cumulative CPU 100.78 sec\n",
      "2019-09-28 21:31:17,080 Stage-1 map = 2%,  reduce = 0%, Cumulative CPU 128.11 sec\n"
     ]
    }
   ],
   "source": [
    "!hive -f task1_watch_new_table.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1ySBr38iJ5t7"
   },
   "outputs": [],
   "source": [
    "%%writefile task1_watch_new_table.hql\n",
    "-- Your code here\n",
    "SELECT * FROM demodb.posts_sample LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B6PHsnNTJ5t9"
   },
   "outputs": [],
   "source": [
    "!hive -f task1_watch_new_table.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qpvRyrvJ5t_"
   },
   "source": [
    "Take the third row of the dataset in the ascending order for the posts (firstly by year, after that by month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IptrKDK8J5t_"
   },
   "outputs": [],
   "source": [
    "%%writefile task1_result.hql\n",
    "-- Your code here\n",
    "use demodb;\n",
    "select concat_ws(\"\\t\",year,m,line_count)\n",
    "from\n",
    "(select year, concat(year,\"-\",month) as m,count(*) as line_count,\n",
    " ROW_NUMBER() OVER (ORDER BY year,month) as row\n",
    "from POSTS_SAMPLE\n",
    "group by year,month\n",
    ") as posts\n",
    "where row=3;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tJin-nhkJ5uB"
   },
   "outputs": [],
   "source": [
    "!hive -f task1_result.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJTsCVTkJ5uC"
   },
   "source": [
    "## Step 5. Submission part. Do not touch!! And simple run all cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k90P-CwSJ5uC"
   },
   "source": [
    "Copy your notebook from the steps <a href=\"#Step-4.-Complete-the-assignment\">Step 4</a> and <a href=\"#Step-5.-Submission-part.-Do-not-touch!!-And-simple-run-all-cells-below!\">Step 5</a> to the new notebook. Run all the cells! And submit the copied notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "76fguZmgJ5uD"
   },
   "outputs": [],
   "source": [
    "!cat task1_create_external_table.hql > task1.hql\n",
    "!cat task1_create_managed_table.hql >> task1.hql\n",
    "!cat task1_insert_table.hql >> task1.hql\n",
    "!cat task1_result.hql >> task1.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KK3zjw7EJ5uF"
   },
   "source": [
    "Take a look at your submission query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CLawbwF4J5uG"
   },
   "outputs": [],
   "source": [
    "!cat task1.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GGO0BjtzJ5uI"
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "$(document).ready(function() {\n",
    "    console.log('Ready');\n",
    "    \n",
    "    \n",
    "    function is_hive_command(list_tokens) {\n",
    "        return list_tokens.indexOf('hive') > -1 && \n",
    "             list_tokens.indexOf('f') > -1 &&\n",
    "             list_tokens.indexOf('-') > -1 && \n",
    "             list_tokens.indexOf('!') > -1 &&\n",
    "             list_tokens.indexOf('hql') > -1 && \n",
    "             list_tokens.indexOf('writefile') == -1;\n",
    "    } \n",
    "    \n",
    "    function collectText(input_tag) {\n",
    "\n",
    "        var result_string = [];\n",
    "        $.each($(input_tag).children(), function(index, child) {\n",
    "            result_string.push($(child).text());\n",
    "        });\n",
    "        return [result_string, is_hive_command(result_string)];\n",
    "    };\n",
    "    \n",
    "    var filtered_results = $(\".cell.code_cell.rendered\").filter(function(index, element) {\n",
    "        var out = collectText($(element).find('.CodeMirror-line').find('span'));\n",
    "        console.log(out);\n",
    "        return collectText($(element).find('.CodeMirror-line').find('span'))[1];\n",
    "    });\n",
    "    $(filtered_results).remove();\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "unTiTnY_J5uK"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hive -f task1.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwS_Hbx3J5uL"
   },
   "source": [
    "Congratulations! You have completed the assignment! Now you can submit it to the system and get your results!\n",
    "\n",
    "Copy your notebook from the steps <a href=\"#Step-4.-Complete-the-assignment\">Step 4</a> and <a href=\"#Step-5.-Submission-part.-Do-not-touch!!-And-simple-run-all-cells-below!\">Step 5</a> to the new notebook. Run all the cells! And submit the copied notebook!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "901_to_students.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
