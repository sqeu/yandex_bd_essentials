{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_schema = StructType([\n",
    "    StructField(\"to\", IntegerType(), False),\n",
    "    StructField(\"from\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_schema = StructType([\n",
    "    StructField(\"vertex\", IntegerType(), False),\n",
    "    StructField(\"distance\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edge(s):\n",
    "  user, follower = s.split(\"\\t\")\n",
    "  return (int(user), int(follower))\n",
    "\n",
    "def step(item):\n",
    "  prev_v, prev_d, next_v = item[0], item[1][0], item[1][1]\n",
    "  return (next_v, prev_d + 1)\n",
    "\n",
    "def complete(item):\n",
    "  v, old_d, new_d = item[0], item[1][0], item[1][1]\n",
    "  return (v, old_d if old_d is not None else new_d)\n",
    "\n",
    "n = 400  # number of partitions\n",
    "edges = sc.textFile(\"/data/twitter/twitter_sample.txt\").map(parse_edge).cache()\n",
    "forward_edges = edges.map(lambda e: (e[1], e[0])).partitionBy(n).persist()\n",
    "\n",
    "x = 12\n",
    "d = 0\n",
    "distances = sc.parallelize([(x, d)]).partitionBy(n)\n",
    "while True:\n",
    "  candidates = distances.join(forward_edges, n).map(step)\n",
    "  new_distances = distances.fullOuterJoin(candidates, n).map(complete, True).persist()\n",
    "  count = new_distances.filter(lambda i: i[1] == d + 1).count()\n",
    "  if count > 0:\n",
    "    d += 1\n",
    "    distances = new_distances\n",
    "    print(\"d = \", d, \"count = \", count)\n",
    "  else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = spark.read.csv(\"/data/twitter/twitter_sample2.txt\", sep=\"\\t\", schema=graph_schema).cache().alias(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| to|from|\n",
      "+---+----+\n",
      "| 12|  18|\n",
      "| 12|  41|\n",
      "+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = edges.where(\"from=12\").withColumn(\"distance\",lit(0)).withColumnRenamed(\"to\",\"to_d\").withColumnRenamed(\"from\",\"from_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_distances = distances.join(edges,distances[\"to_d\"] == edges[\"from\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+---+----+\n",
      "|to_d|from_d|distance| to|from|\n",
      "+----+------+--------+---+----+\n",
      "| 380|    12|       0|690| 380|\n",
      "| 380|    12|       0| 31| 380|\n",
      "+----+------+--------+---+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_distances.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shortest_path(v_from, v_to, dataset_path=None):\n",
    "    edges = spark.read.csv(dataset_path, sep=\"\\t\", schema=graph_schema)\n",
    "    path_length = None\n",
    "    \n",
    "    # Here you should implement the BFS algorithm. It should return the length\n",
    "    # of the minimal path (single integer) between v_from and v_to\n",
    "    \n",
    "    return path_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print shortest_path(12, 34, \"/data/twitter/twitter_sample2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
