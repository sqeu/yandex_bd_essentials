{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_m = data.limit(5000).drop(\"artistId\")\n",
    "data_b = data.withColumnRenamed(\"trackId\",\"b_trackId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data=data.alias(\"a\").join(data_b.alias(\"b\"),\"userId\")\\\n",
    ".where(\"a.timestamp!=b.timestamp and trackId!=b_trackId\")\\\n",
    ".where(\"b.timestamp - a.timestamp < 420 and b.timestamp - a.timestamp >= 0 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "window = Window.partitionBy(col(\"trackId\")).orderBy(col(\"count\"))\n",
    "\n",
    "track = train_data.groupBy(col(\"trackId\"),col(\"b_trackId\")).count()\\\n",
    "        .withColumn(\"position\", rank().over(window))\\\n",
    "        .filter(col(\"position\") < 50)\n",
    "\n",
    "trackNorm = norm(track, \"trackId\", \"b_trackId\", \"count\", 1000) \\\n",
    "        .withColumn(\"id\", col(\"trackId\")) \\\n",
    "        .withColumn(\"id2\", col(\"b_trackId\")) \\\n",
    "        .withColumn(\"norm_count\", col(\"norm_count\") * 0.5) \\\n",
    "        .select(col(\"id\"), col(\"id2\"), col(\"norm_count\"))     \n",
    "\n",
    "window = Window.orderBy(col(\"norm_count\").desc())\n",
    "    \n",
    "trackList = trackNorm.withColumn(\"position2\", rank().over(window))\\\n",
    "    .orderBy(desc(\"norm_count\"),col(\"id\"), col(\"id2\"))\\\n",
    "    .select(col(\"id\"), col(\"id2\"))\\\n",
    "    .take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798256 923706\n",
      "798258 808254\n",
      "798302 836228\n",
      "798322 876562\n",
      "798331 827364\n",
      "798335 840741\n",
      "798376 888871\n",
      "798379 812055\n",
      "798398 926302\n",
      "798403 868805\n",
      "798405 867217\n",
      "798426 910880\n",
      "798447 832635\n",
      "798457 918918\n",
      "798471 801831\n",
      "798474 963162\n",
      "798475 827475\n",
      "798505 905671\n",
      "798508 810743\n",
      "798516 860347\n",
      "798526 937573\n",
      "798542 946408\n",
      "798544 841232\n",
      "798550 936295\n",
      "798552 830267\n",
      "798618 930224\n",
      "798667 874844\n",
      "798682 934393\n",
      "798704 937570\n",
      "798707 839389\n",
      "798720 958333\n",
      "798725 933147\n",
      "798731 853117\n",
      "798782 956938\n",
      "798801 950802\n",
      "798820 890393\n",
      "798821 883244\n",
      "798827 908022\n",
      "798851 801321\n",
      "798978 854212\n"
     ]
    }
   ],
   "source": [
    "for val in trackList:\n",
    "    print \"%s %s\" % val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
