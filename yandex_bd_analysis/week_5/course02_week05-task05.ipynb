{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-------+\n",
      "| type|                Name|              Artist|     Id|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "|track|               Smile| Artist: Josh Groban|1223851|\n",
      "|track|Chuni Ashkharhe Q...|Artist: Razmik Amyan|1215486|\n",
      "+-----+--------------------+--------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+----------+\n",
      "|userId|trackId|artistId| timestamp|\n",
      "+------+-------+--------+----------+\n",
      "| 13065| 944906|  978428|1501588527|\n",
      "|101897| 799685|  989262|1501555608|\n",
      "+------+-------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum,desc,when,col, rank,lit\n",
    "\n",
    "user = data.where(\"userId=776748\")\\\n",
    "    .select(\"artistId\",\"trackId\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[artistId: int, trackId: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artists = user.join(meta,\"user.trackId==meta.Id\")\n",
    "artists.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967993 869415\n",
      "967998 947428\n",
      "968004 927380\n",
      "968017 859321\n",
      "968022 852786\n",
      "968034 807671\n",
      "968038 964150\n",
      "968042 835935\n",
      "968043 913568\n",
      "968046 935077\n",
      "968047 806127\n",
      "968065 907906\n",
      "968073 964586\n",
      "968086 813446\n",
      "968092 837129\n",
      "968118 914441\n",
      "968125 821410\n",
      "968140 953008\n",
      "968148 877445\n",
      "968161 809793\n",
      "968163 803065\n",
      "968168 876119\n",
      "968189 858639\n",
      "968221 896937\n",
      "968224 892880\n",
      "968232 825536\n",
      "968237 932845\n",
      "968238 939177\n",
      "968241 879045\n",
      "968242 911250\n",
      "968248 953554\n",
      "968255 808494\n",
      "968259 880230\n",
      "968265 950148\n",
      "968266 824437\n",
      "968269 913243\n",
      "968272 816049\n",
      "968278 946743\n",
      "968285 847460\n",
      "968286 940006\n"
     ]
    }
   ],
   "source": [
    "for val in userList:\n",
    "    print \"%s %s\" % val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
